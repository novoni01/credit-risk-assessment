{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edacb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "#gather data from a specific csv file and return as a pandas df\n",
    "BASE = Path.cwd()\n",
    "DATA = BASE / \"training_data\"\n",
    "#Make sure training_data folder exists\n",
    "DATA.mkdir(parents= True, exist_ok= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d5236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the user path \n",
    "user_path = os.path.expanduser(\"~\")\n",
    "#Make the path to the kaggle json\n",
    "downloads_path = os.path.join(user_path, 'Downloads/kaggle.json')\n",
    "\n",
    "#Make the .kaggle folder in user\n",
    "kaggle_folder = os.path.join(user_path, \".kaggle\")\n",
    "os.makedirs(kaggle_folder, exist_ok= True)\n",
    "\n",
    "print(os.path.exists(downloads_path))\n",
    "\n",
    "if os.path.exists(downloads_path):\n",
    "    try:\n",
    "        print(\"moving kaggle.json to user\")\n",
    "        #Make .kaggle folder path and check it exists\n",
    "        \n",
    "        #Move json to the folder\n",
    "        shutil.move(downloads_path, kaggle_folder)\n",
    "    except Exception as e:\n",
    "        print(f\"Error when trying to move kaggle.json: {e}\")\n",
    "else:\n",
    "    #attempt to check if kaggle.json already exists\n",
    "    final = os.path.join(kaggle_folder, \"kaggle.json\")\n",
    "    if os.path.exists(final):\n",
    "        print(\"kaggle json already exists in user\")\n",
    "    else:\n",
    "        print(\"please download kaggle.json\")\n",
    "\n",
    "#print(\"Successfully moved json file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad2426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO USE THIS API you must have a .kaggle folder in your 'C:\\NAME' directory -> then paste the kaggle.json authenticator\n",
    "import kaggle\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "print(api.dataset_list_files('wordsforthewise/lending-club').files)\n",
    "\n",
    "api.dataset_download_files('wordsforthewise/lending-club', path= DATA, unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_training_csv(): \n",
    "    \"\"\" Function that returns all the csv files in the training data folder as a dataframe object \"\"\" \n",
    "    csv_list = list(DATA.glob(\"**/*.csv\")) \n",
    "    return_list = [] \n",
    "    print(csv_list)\n",
    "\n",
    "    #traverse each item inthe data path, and if a file ending in .csv is found, turn it into a df and append to return list \n",
    "    for item in csv_list: \n",
    "        if os.path.isfile(item): \n",
    "            print(f\"{item} is a file\") \n",
    "            return_list.append(pd.read_csv(item)) \n",
    "        else: print(f\"{item} is folder or dir\")\n",
    "\n",
    "    if return_list: \n",
    "        return return_list \n",
    "    else: \n",
    "        print(f\"No csv files found in {DATA}\") \n",
    "        return []\n",
    "\n",
    "def get_dir_size(path): \n",
    "    \"\"\" Get directory size in MBs \"\"\" \n",
    "    total = 0 \n",
    "    for dirpath, dirs, files in os.walk(path):\n",
    "        for f in files:\n",
    "            file_path = os.path.join(dirpath, f)\n",
    "            \n",
    "            try:\n",
    "                total += os.path.getsize(file_path) \n",
    "                print(f\"{total/1000000} MB\")\n",
    "            except: \n",
    "                continue \n",
    "        return total/1000000\n",
    "    \n",
    "def delete_large_files(): \n",
    "    \"\"\" Function that deletes the large files downloaded from kaggle to save space \n",
    "    Deletes files to prevent any storage errors when pushing code to github \"\"\"\n",
    "\n",
    "    for paths in DATA.glob(\"**/*\"): \n",
    "        if os.path.isdir(paths): \n",
    "            try:\n",
    "                size = get_dir_size(paths) \n",
    "                print(f\"size of {paths} is {size}\")\n",
    "\n",
    "                if size > 100: \n",
    "                    print(f\"deleting {paths}\") \n",
    "                    shutil.rmtree(paths) \n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error when trying to get size or delete file: {e}\")\n",
    "        else:\n",
    "            if paths.suffix == \".gz\":\n",
    "                os.remove(paths)\n",
    "                #print(paths)\n",
    "        \n",
    "\n",
    "df_list = retrieve_training_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a40853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create different pointers to the csvs in the list for easier access\n",
    "raw_accepted_loans = df_list[0]\n",
    "raw_rejected_loans = df_list[1]\n",
    "raw_loans_paid_info = df_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1390ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_accepted_loans.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5177324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "al_columns = list(raw_accepted_loans.columns)\n",
    "print(al_columns)\n",
    "raw_accepted_loans['loan_status'].head(10)\n",
    "num_cols = [\n",
    "    \"id\", \"loan_amnt\", \"funded_amnt\", \"term\", \"int_rate\", \"installment\", \"annual_inc\",\n",
    "    \"dti\", \"delinq_2yrs\", \"fico_range_low\", \"fico_range_high\", \"inq_last_6mths\",\n",
    "    \"open_acc\", \"revol_bal\", \"revol_util\", \"total_acc\", \"pub_rec_bankruptcies\"\n",
    "]\n",
    "\n",
    "text_cols = [\n",
    "    \"home_ownership\", \"loan_status\", \"purpose\", \"application_type\", \"verification_status\"\n",
    "]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "raw_specific_al = raw_accepted_loans[target_columns]\n",
    "raw_specific_al.head(10).T\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d336486",
   "metadata": {},
   "source": [
    "**Clean Accepted Loans Data**\n",
    "- Provide a data summary\n",
    "- Standardize all data in the columns to reasonable types (ex: object -> float)\n",
    "- Clean up any bad data (missing values, duplicates, etc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bab88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only find columns with valid ids (not string or empty)\n",
    "cleaned_accepted_loans = raw_accepted_loans[num_cols + text_cols]\n",
    "cleaned_accepted_loans = cleaned_accepted_loans[pd.to_numeric(cleaned_accepted_loans['id'], errors='coerce').notna()]\n",
    "\n",
    "#convert id column to int instead of obj\n",
    "cleaned_accepted_loans['id'] = cleaned_accepted_loans['id'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix the the term column to make it float (and in months)\n",
    "cleaned_accepted_loans.rename(columns = {'term': 'term_months'}, inplace = True)\n",
    "num_cols[3] = 'term_months'\n",
    "cleaned_accepted_loans['term_months'] = cleaned_accepted_loans['term_months'].str.replace(\" months\", \"\")\n",
    "cleaned_accepted_loans['term_months'] = cleaned_accepted_loans['term_months'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30848913",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_accepted_loans.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d7a3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find any na rows and fill them as needed\n",
    "for rows in num_cols:\n",
    "    cleaned_accepted_loans.loc[:, rows] = cleaned_accepted_loans[rows].fillna(cleaned_accepted_loans[rows].median())\n",
    "\n",
    "cleaned_accepted_loans.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d1a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a sample of the cleaned up \n",
    "sampled_accepted_loans = cleaned_accepted_loans.sample(200000, random_state = 821)\n",
    "sampled_accepted_loans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097496c4",
   "metadata": {},
   "source": [
    "**Clean Rejected Loan Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639207ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_rejected_loans = raw_rejected_loans.copy()\n",
    "cleaned_rejected_loans.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unneeded columns \n",
    "cleaned_rejected_loans = cleaned_rejected_loans.drop(columns=['Risk_Score', 'Zip Code', 'State', 'Policy Code', 'Employment Length'])\n",
    "rl_cols = list(cleaned_rejected_loans.columns)\n",
    "print(rl_cols)\n",
    "\n",
    "cleaned_rejected_loans.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f9677",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(cleaned_rejected_loans['Amount Requested'].dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix any missing values in the rejection loans\n",
    "for rows in rl_cols:\n",
    "    #fix the rows that are of float type\n",
    "    if cleaned_rejected_loans[rows].dtypes == 'float64':\n",
    "        print(f\"replacing {rows} with median float\")\n",
    "        cleaned_rejected_loans.loc[:, rows] = cleaned_rejected_loans[rows].fillna(cleaned_rejected_loans[rows].median())\n",
    "    else:\n",
    "        #get the highest repeated string and replace N/A's with that string\n",
    "        print(f\"replacing {rows} na's with most repeated string\")\n",
    "        replacement_string = cleaned_rejected_loans[rows].value_counts().reset_index().at[0,rows]\n",
    "        cleaned_rejected_loans.loc[:, rows] = cleaned_rejected_loans[rows].fillna(replacement_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_rejected_loans.isna().sum()\n",
    "print(len(cleaned_rejected_loans))\n",
    "\n",
    "cleaned_rejected_loans.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b1c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get csv of accepted loans from 2023 HMDA\n",
    "import requests\n",
    "try:\n",
    "    #response = requests.get(\"https://ffiec.cfpb.gov/v2/data-browser-api/view/nationwide/csv?years=2023&actions_taken=1&loan_purpose=1\")\n",
    "    accepted_response = requests.get(\"https://files.ffiec.cfpb.gov/data-browser/datasets/2023/filtered-queries/one-year/a5a77b5e9528ccb95aae0dc60cea9d70.csv\", stream= True)\n",
    "    accepted_response.raise_for_status()\n",
    "except requests.HTTPError as e:\n",
    "    print(f\"Error getting HDMI data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71accde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning response into csv\n",
    "import io\n",
    "import csv\n",
    "raw_string = accepted_response.text\n",
    "split_string = raw_string.split('\\n')\n",
    "#write to a csv in the training data folder\n",
    "string_columns = split_string[0].split(',')\n",
    "string_data = [rows.split(',') for rows in split_string[1:]]\n",
    "\n",
    "hdma_path = DATA / 'hdma_accepted_raw.csv'\n",
    "\n",
    "with open(hdma_path, \"w\", newline=\"\") as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(string_columns)\n",
    "    csv_writer.writerows(string_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f8c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import csv\n",
    "hdma_path = DATA / 'hdma_accepted_raw.csv'\n",
    "space_saver = pd.read_csv(hdma_path, low_memory= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7d6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdma_accepted_parquet = DATA / 'hdma_accepted_raw.parquet.gzip'\n",
    "#space_saver.to_parquet(hdma_accepted_parquet)\n",
    "for columns in space_saver.columns:\n",
    "    if space_saver[columns].dtype == 'object':\n",
    "        print('object')\n",
    "        space_saver[columns] = space_saver[columns].astype('string')\n",
    "    else:\n",
    "        print(space_saver[columns].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce063ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns that do not affect model predictions in any meaningful way. dropped in order to save space when uploading to repository\n",
    "drop_cols = [\n",
    "    'tract_population', 'tract_minority_population_percent', 'ffiec_msa_md_median_family_income', 'tract_to_msa_income_percentage',\n",
    "    'tract_owner_occupied_units', 'tract_one_to_four_family_homes', 'tract_median_age_of_housing_units', 'aus-1', 'aus-2', 'aus-3', 'aus-4',\n",
    "    'aus-5', 'applicant_ethnicity-1', 'applicant_ethnicity-2', 'applicant_ethnicity-3', 'applicant_ethnicity-4', 'applicant_ethnicity-5',\n",
    "    'co-applicant_ethnicity-1', 'co-applicant_ethnicity-2', 'co-applicant_ethnicity-3', 'co-applicant_ethnicity-4', 'co-applicant_ethnicity-5',\n",
    "    'applicant_ethnicity_observed', 'co-applicant_ethnicity_observed', 'applicant_race-1', 'applicant_race-2', 'applicant_race-3', 'applicant_race-4',\n",
    "    'applicant_race-5', 'co-applicant_race-1', 'co-applicant_race-2', 'co-applicant_race-3', 'co-applicant_race-4', 'co-applicant_race-5',\n",
    "    'applicant_race_observed', 'co-applicant_race_observed', 'applicant_sex', 'co-applicant_sex', 'applicant_sex_observed', 'co-applicant_sex_observed',\n",
    "    'applicant_age', 'co-applicant_age', 'applicant_age_above_62', 'co-applicant_age_above_62'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba3e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "space_saver.drop(columns = drop_cols).to_parquet(hdma_accepted_parquet, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8127163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered = pd.read_parquet(hdma_accepted_parquet, columns=temp_num_cols + temp_text_cols)\n",
    "\n",
    "#print(temp_num_cols + temp_text_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdd7216",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f9dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_index = []\n",
    "\n",
    "#get the column numbers needed\n",
    "for columns in temp_num_cols:\n",
    "    remove_index.append(string_columns.index(columns))\n",
    "\n",
    "for columns in temp_text_cols:\n",
    "    remove_index.append(string_columns.index(columns))\n",
    "\n",
    "print(remove_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13815a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_cols = [row for row in string_columns if string_columns.index(row) in remove_index]\n",
    "finalized_text = [row for row in string_data if string_data.index(row) in remove_index]\n",
    "print((finalized_cols))\n",
    "print(len(string_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b5f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = [i for i in range(0,100) if i not in remove_index]\n",
    "print(temp_list)\n",
    "split_string.clear()\n",
    "print(len(temp_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df04047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string_columns)\n",
    "print(string_data[0])\n",
    "hmda_accepted = pd.DataFrame(string_data, columns=string_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58e0f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://ffiec.cfpb.gov/documentation/publications/loan-level-datasets/lar-data-fields#loan_amount\n",
    "temp_num_cols = [\n",
    "    'activity_year', 'action_taken', 'preapproval', 'loan_purpose', 'loan_amount', 'loan_to_value_ratio',\n",
    "    'loan_term', 'income', 'debt_to_income_ratio'\n",
    "    ]\n",
    "temp_text_cols = [\n",
    "    'derived_loan_product_type', 'applicant_credit_score_type', 'co-applicant_credit_score_type', 'denial_reason-1'\n",
    "    ]\n",
    "\n",
    "\"\"\"\n",
    "num_cols = [\n",
    "    \"id\", \"loan_amnt\", \"funded_amnt\", \"term\", \"int_rate\", \"installment\", \"annual_inc\",\n",
    "    \"dti\", \"delinq_2yrs\", \"fico_range_low\", \"fico_range_high\", \"inq_last_6mths\",\n",
    "    \"open_acc\", \"revol_bal\", \"revol_util\", \"total_acc\", \"pub_rec_bankruptcies\"\n",
    "]\n",
    "\n",
    "text_cols = [\n",
    "    \"home_ownership\", \"loan_status\", \"purpose\", \"application_type\", \"verification_status\"\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d96a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmda_accepted.iloc[0:5,20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c7ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmda_accepted = hmda_accepted[temp_num_cols + temp_text_cols]\n",
    "hmda_accepted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmda_accepted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f591ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get csv of rejected loans from 2023 HMDA\n",
    "import requests\n",
    "try:\n",
    "    rejected_response = requests.get(\"https://ffiec.cfpb.gov/v2/data-browser-api/view/nationwide/csv?years=2023&actions_taken=3&loan_purpose=1\")\n",
    "    rejected_response.raise_for_status()\n",
    "except requests.HTTPError as e:\n",
    "    print(f\"Error getting HDMI data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47866d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected_string = rejected_response.text\n",
    "split_string = rejected_string.split('\\n')\n",
    "#write to a csv in the training data folder\n",
    "rejected_columns = split_string[0].split(',')\n",
    "rejected_data = [rows.split(',') for rows in split_string[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e219b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rejected_columns))\n",
    "print(rejected_data[0])\n",
    "hdma_rejected_path = DATA / 'hdma_rejected_raw.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9420011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write raw data to unique csv\n",
    "import csv\n",
    "with open(hdma_rejected_path, \"w\", newline=\"\") as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(rejected_columns)\n",
    "    csv_writer.writerows(rejected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae695425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporarily read csv, drop unneeded columns, and parquet the file to save space\n",
    "rejected_temp = pd.read_csv(hdma_rejected_path, low_memory= False)\n",
    "\n",
    "hdma_rejected_parquet = DATA / 'hdma_rejected_raw.parquet.gzip'\n",
    "#space_saver.to_parquet(hdma_accepted_parquet)\n",
    "for columns in rejected_temp.columns:\n",
    "    if rejected_temp[columns].dtype == 'object':\n",
    "        print('object')\n",
    "        rejected_temp[columns] = rejected_temp[columns].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc90c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected_temp.drop(columns = drop_cols).to_parquet(hdma_rejected_parquet, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74efb7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdma_accepted_parquet = DATA / 'hdma_accepted_raw.parquet.gzip'\n",
    "hdma_rejected_parquet = DATA / 'hdma_rejected_raw.parquet.gzip'\n",
    "accepted_recovered = pd.read_parquet(hdma_accepted_parquet, columns=temp_num_cols + temp_text_cols)\n",
    "rejected_recovered = pd.read_parquet(hdma_rejected_parquet, columns=temp_num_cols + temp_text_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347c1ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_recovered.isna().sum()\n",
    "accepted_recovered.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a85d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected_recovered.isna().sum()\n",
    "rejected_recovered.head(10)\n",
    "#rejected_recovered.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ae547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = 'loan_term'\n",
    "print(rejected_recovered[test_name].value_counts().reset_index())\n",
    "#print(rejected_recovered[test_name].value_counts().reset_index().iat[0, 0])\n",
    "\n",
    "int_values = [\n",
    "    'activity_year', 'action_taken', 'preapproval', 'loan_purpose', 'loan_amount', 'loan_term', 'applicant_credit_score_type',\n",
    "    'co-applicant_credit_score_type', 'denial_reason-1'\n",
    "]\n",
    "\n",
    "float_values = [\n",
    "    'loan_to_value_ratio', 'income', 'debt_to_income_ratio'\n",
    "]\n",
    "\n",
    "string_values = [\n",
    "    'derived_loan_product_type'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b9072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert any ranges into the middle of that range, or leave it at that range if its too broad\n",
    "test_name = 'debt_to_income_ratio'\n",
    "ranges = rejected_recovered[test_name].value_counts().reset_index()\n",
    "\n",
    "rejected_recovered[test_name] = rejected_recovered[test_name].replace(\">60%\", 60)\n",
    "rejected_recovered[test_name] = rejected_recovered[test_name].replace(\"50%-60%\", 55)\n",
    "rejected_recovered[test_name] = rejected_recovered[test_name].replace(\"20%-<30%\", (29+20)/2)\n",
    "rejected_recovered[test_name] = rejected_recovered[test_name].replace(\"30%-<36%\", (35+30)/2)\n",
    "rejected_recovered[test_name] = rejected_recovered[test_name].replace(\"<20%\", 20)\n",
    "\n",
    "rejected_recovered[test_name].value_counts().reset_index()\n",
    "\n",
    "#rejected_recovered[column] = rejected_recovered[column].astype('float64', errors=\"ignore\")\n",
    "\n",
    "rejected_recovered[test_name] = rejected_recovered[test_name].replace('Exempt', rejected_recovered[test_name].value_counts().reset_index().iat[0, 0])\n",
    "\n",
    "rejected_recovered[test_name].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e9a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = 'loan_term'\n",
    "rejected_recovered[test_name] = rejected_recovered[test_name].replace('Exempt', rejected_recovered[test_name].value_counts().reset_index().iat[0, 0])\n",
    "\n",
    "rejected_recovered[test_name].value_counts().reset_index()\n",
    "print(len(rejected_recovered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e4f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = 'income'\n",
    "rejected_recovered[test_name] = rejected_recovered[test_name].replace('Exempt', rejected_recovered[test_name].value_counts().reset_index().iat[0, 0])\n",
    "\n",
    "rejected_recovered[test_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25550c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = 'loan_to_value_ratio'\n",
    "#fix any rows that have exempt\n",
    "\n",
    "rejected_recovered[test_name] = rejected_recovered[test_name].replace('Exempt', rejected_recovered[test_name].value_counts().reset_index().iat[0, 0])\n",
    "\n",
    "rejected_recovered[test_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817fd742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up dataframe and turn it into proper typings\n",
    "for column in int_values:\n",
    "    print(f\"changing column: {column}\")\n",
    "    rejected_recovered[column] = rejected_recovered[column].fillna(rejected_recovered[column].value_counts().reset_index().iat[0, 0])\n",
    "    try:\n",
    "        rejected_recovered[column] = rejected_recovered[column].astype('int32')\n",
    "    except Exception as e:\n",
    "        print(f\"skipping int conversion: {column}\") \n",
    "        print(f\"reason: {e}\")\n",
    "        continue  \n",
    "    \n",
    "\n",
    "for column in float_values:\n",
    "    print(f\"converting float: {column}\")\n",
    "    rejected_recovered[column] = rejected_recovered[column].fillna(rejected_recovered[column].value_counts().reset_index().iat[0, 0])   \n",
    "    try:\n",
    "        rejected_recovered[column] = rejected_recovered[column].astype('float64')\n",
    "    except Exception as e:\n",
    "        print(f\"skipping float conversion: {column}\")\n",
    "        print(f\"reason: {e}\")\n",
    "\n",
    "for column in string_values:\n",
    "    rejected_recovered[column] = rejected_recovered[column].fillna(rejected_recovered[column].value_counts().reset_index().iat[0, 0])\n",
    "\n",
    "rejected_recovered.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cbf26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "for column in rejected_recovered.columns:\n",
    "    if rejected_recovered[column].dtype == 'float64':\n",
    "\"\"\"\n",
    "\n",
    "rejected_recovered.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5740889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove outliers from rejected df based on loan amount\n",
    "rejected_recovered\n",
    "\n",
    "\"\"\"\n",
    "int_values = [\n",
    "    'activity_year', 'action_taken', 'preapproval', 'loan_purpose', 'loan_amount', 'loan_term', 'applicant_credit_score_type',\n",
    "    'co-applicant_credit_score_type', 'denial_reason-1'\n",
    "]\n",
    "\n",
    "float_values = [\n",
    "    'loan_to_value_ratio', 'income', 'debt_to_income_ratio'\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "loan_std = rejected_recovered['loan_amount'].std()\n",
    "loan_mean = rejected_recovered['loan_amount'].mean()\n",
    "print(loan_std)\n",
    "print(loan_mean)\n",
    "temp_z_data = rejected_recovered.copy()\n",
    "temp_z_data['z_loan'] = ((rejected_recovered['loan_amount'] - loan_mean) / loan_std)\n",
    "threshold = 3\n",
    "\n",
    "#remove based on threshold\n",
    "no_outliers_rejected = temp_z_data[(temp_z_data['z_loan'].abs()) <= threshold].drop(columns=['z_loan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a3a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_outliers_rejected['loan_amount'].value_counts().reset_index()\n",
    "print(len(no_outliers_rejected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788a2d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up accepted hdma loan (repeat of the rejected clean up process)\n",
    "#Convert any ranges into the middle of that range, or leave it at that range if its too broad\n",
    "test_name = 'debt_to_income_ratio'\n",
    "ranges = accepted_recovered[test_name].value_counts().reset_index()\n",
    "\n",
    "accepted_recovered[test_name] = accepted_recovered[test_name].replace(\">60%\", 60)\n",
    "accepted_recovered[test_name] = accepted_recovered[test_name].replace(\"50%-60%\", 55)\n",
    "accepted_recovered[test_name] = accepted_recovered[test_name].replace(\"20%-<30%\", (29+20)/2)\n",
    "accepted_recovered[test_name] = accepted_recovered[test_name].replace(\"30%-<36%\", (35+30)/2)\n",
    "accepted_recovered[test_name] = accepted_recovered[test_name].replace(\"<20%\", 20)\n",
    "\n",
    "accepted_recovered[test_name].value_counts().reset_index()\n",
    "\n",
    "#accepted_recovered[column] = accepted_recovered[column].astype('float64', errors=\"ignore\")\n",
    "\n",
    "accepted_recovered[test_name] = accepted_recovered[test_name].replace('Exempt', accepted_recovered[test_name].value_counts().reset_index().iat[0, 0])\n",
    "\n",
    "accepted_recovered[test_name].value_counts().reset_index()\n",
    "\n",
    "test_name = 'loan_term'\n",
    "accepted_recovered[test_name] = accepted_recovered[test_name].replace('Exempt', accepted_recovered[test_name].value_counts().reset_index().iat[0, 0])\n",
    "\n",
    "accepted_recovered[test_name].value_counts().reset_index()\n",
    "print(len(accepted_recovered))\n",
    "\n",
    "test_name = 'income'\n",
    "accepted_recovered[test_name] = accepted_recovered[test_name].replace('Exempt', accepted_recovered[test_name].value_counts().reset_index().iat[0, 0])\n",
    "\n",
    "accepted_recovered[test_name].value_counts()\n",
    "\n",
    "test_name = 'loan_to_value_ratio'\n",
    "#fix any rows that have exempt\n",
    "\n",
    "accepted_recovered[test_name] = accepted_recovered[test_name].replace('Exempt', accepted_recovered[test_name].value_counts().reset_index().iat[0, 0])\n",
    "\n",
    "accepted_recovered[test_name]\n",
    "\n",
    "#clean up dataframe and turn it into proper typings\n",
    "for column in int_values:\n",
    "    print(f\"changing column: {column}\")\n",
    "    accepted_recovered[column] = accepted_recovered[column].fillna(accepted_recovered[column].value_counts().reset_index().iat[0, 0])\n",
    "    try:\n",
    "        accepted_recovered[column] = accepted_recovered[column].astype('int32')\n",
    "    except Exception as e:\n",
    "        print(f\"skipping int conversion: {column}\") \n",
    "        print(f\"reason: {e}\")\n",
    "        continue  \n",
    "    \n",
    "\n",
    "for column in float_values:\n",
    "    print(f\"converting float: {column}\")\n",
    "    accepted_recovered[column] = accepted_recovered[column].fillna(accepted_recovered[column].value_counts().reset_index().iat[0, 0])   \n",
    "    try:\n",
    "        accepted_recovered[column] = accepted_recovered[column].astype('float64')\n",
    "    except Exception as e:\n",
    "        print(f\"skipping float conversion: {column}\")\n",
    "        print(f\"reason: {e}\")\n",
    "\n",
    "for column in string_values:\n",
    "    accepted_recovered[column] = accepted_recovered[column].fillna(accepted_recovered[column].value_counts().reset_index().iat[0, 0])\n",
    "\n",
    "loan_std = accepted_recovered['loan_amount'].std()\n",
    "loan_mean = accepted_recovered['loan_amount'].mean()\n",
    "print(loan_std)\n",
    "print(loan_mean)\n",
    "temp_z_data = accepted_recovered.copy()\n",
    "temp_z_data['z_loan'] = ((accepted_recovered['loan_amount'] - loan_mean) / loan_std)\n",
    "threshold = 3\n",
    "\n",
    "#remove based on threshold\n",
    "no_outliers_accepted = temp_z_data[(temp_z_data['z_loan'].abs()) <= threshold].drop(columns=['z_loan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete_large_files()\n",
    "\n",
    "no_outliers_accepted.head(10)\n",
    "no_outliers_accepted.isna().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
